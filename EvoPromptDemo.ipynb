{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EvoPrompt Demo\n",
    "> In this notebook we will implement EvoPrompt(GA) with GPT-3.5 using OpenAI's API. This implementation leverages a Genetic Algorithm as the evolutionary operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import openai\n",
    "import rouge_score\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "with open(\"C:/Users/danie/OneDrive/Desktop/openai_youtube_api_key.txt\") as f:\n",
    "    api_key = f.readline()\n",
    "\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at a sample of how to use OpenAI's ChatCompletion endpoint so we know what to give our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to structure a paragraph effectively, there are a few key elements that you should keep in mind. Here is a step-by-step guide on how to structure a paragraph:\n",
      "\n",
      "1. Start with a topic sentence: A topic sentence is the main idea or point that you want to express in your paragraph. It should be clear and concise, stating the main topic or focus of the paragraph. This sentence acts as a roadmap, guiding the reader on what to expect.\n",
      "\n",
      "2. Provide supporting details: After stating your topic sentence, it is important to provide supporting details and evidence to support or explain your main idea. You can include examples, facts, or quotes that illustrate or prove your point. Make sure that the details you include are relevant and help to build a stronger argument or discussion.\n",
      "\n",
      "3. Give explanations or examples: Once you have provided supporting details, it is important to explain or expand upon them. You can do this by offering explanations, analyses, or examples that further clarify your point. This step helps to ensure that your reader understands the relevance and significance of the information you have provided.\n",
      "\n",
      "4. Use transitional words and phrases: To create flow and cohesion in your paragraph, it is important to use transitional words and phrases. These words and phrases help to connect ideas and guide the reader smoothly from one point to the next. Examples of transitional words include \"however,\" \"in addition,\" \"on the other hand,\" and \"therefore\". Their usage helps create logical connections between ideas.\n",
      "\n",
      "5. Conclude the paragraph: The final step in structuring a paragraph is to provide a conclusion or a closing statement. This can be a sentence that summarizes the main point you made in the paragraph or one that leads into the next paragraph. It should provide closure and help transition to the next part of your writing piece smoothly.\n",
      "\n",
      "By following these steps, you can effectively structure your paragraphs, making them organized, cohesive, and easy to understand. Remember, paragraphs are building blocks of your writing, and a well-structured paragraph greatly contributes to the overall clarity and effectiveness of your work.\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sixth grade english teacher.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain how to structure a paragraph.\"}\n",
    "    ]\n",
    ")\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we need to provide a role and a user query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our implementation match our pseudocode as closely as possible:\n",
    "![EvoPrompt(GA)_pseudocode.png](attachment:EvoPrompt(GA)_pseudocode.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddHumanEngineeredPrompts():\n",
    "    messages = []\n",
    "    role = \"You are an AI assistant. You will be given a task. Complete the task and explain in detail how you came to your conclusion.\"#input(\"Please enter a role for GPT-3.5 to assume: \")\n",
    "    user_query = \"\"\"Your task is to determine if the following two sentences convey the same meaning. First, answer either with yes or no, then explain your reasoning. \n",
    "Sentence 1: The dog eagerly dug up the ground to find the bone it had buried yesterday.\n",
    "Sentence 2: Excited to retrieve what it hid, the dog unearthed the bone it stashed away the previous day.\"\"\"#input(\"Please provide a user prompt: \")\n",
    "    messages.append({\"role\": \"system\", \"content\": role})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateRandomPromptsLLM(N, user_prompt):\n",
    "    num_generated = N-1\n",
    "    prompts = []\n",
    "    i = 0\n",
    "    while i < num_generated:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You're an AI assistant who completes its given its task as closely to the prompt as possible.\"}, \n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Your task is to generate a response which is in the same format as the following python list: \\n {user_prompt}\n",
    "                This content sections in the dictionaries within your list should convey the same idea as those in original but make sure your response is in some way different than the original list.\n",
    "                Finally and most importantly, if the example mentions comparing sentences make sure to use the same sentences from the example in your response.\"\"\"}\n",
    "            ]\n",
    "        )\n",
    "        if response['choices'][0]['message']['content'] != user_prompt:\n",
    "            prompts.append(eval(response['choices'][0]['message']['content']))\n",
    "            i += 1\n",
    "        \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_starting_generation(prompts):\n",
    "    print(\"Generation 0 (Initialization Prompts):\")\n",
    "    for i, p in enumerate(prompts):\n",
    "        print(f\"Prompt {i}:\")\n",
    "        print(\"System Role:\", p[0]['content'])\n",
    "        print(\"User Prompt:\", p[1]['content'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generation_best(gen_num, prompts, argmax):\n",
    "    content = prompts[argmax]\n",
    "    print(f\"Generation {gen_num} Best Candidate:\")\n",
    "    print(f\"Prompt {argmax+1}:\")\n",
    "    print(\"System Role:\", content[0]['content'])\n",
    "    print(\"User Prompt:\", content[1]['content'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roulette_wheel_selection(scores):\n",
    "    # Min-Max normalization\n",
    "    min_val = np.min(scores)\n",
    "    max_val = np.max(scores)\n",
    "    normalized_scores = (scores - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Make sure they sum to 1 for probabilities\n",
    "    normalized_scores /= normalized_scores.sum()\n",
    "    \n",
    "    # Randomly select two indices based on their probabilities\n",
    "    selected_indices = np.random.choice(len(scores), 2, replace=False, p=normalized_scores)\n",
    "    \n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evoprompt_ga(num_prompts, num_iterations, target_response):\n",
    "    \n",
    "    #Step 1: Initialize Populations\n",
    "    P = [] # Prompt Population\n",
    "    human_prompt = AddHumanEngineeredPrompts()\n",
    "    hp_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=human_prompt\n",
    "    )\n",
    "    hp_pred = hp_response['choices'][0]['message']['content']\n",
    "    hp_score = rouge_score.compute(\n",
    "        predictions=[hp_pred],\n",
    "        references=[target_response]\n",
    "    )  \n",
    "    P.append(human_prompt)\n",
    "    failed = True\n",
    "    while failed:\n",
    "        try:\n",
    "            model_prompts = GenerateRandomPromptsLLM(num_prompts, P[0])\n",
    "            failed = False\n",
    "        except SyntaxError:\n",
    "            continue\n",
    "        \n",
    "    for prompt in model_prompts:\n",
    "        P.append(prompt)\n",
    "    print_starting_generation(P)\n",
    "    \n",
    "    #Step 2: Evolutionary Loop\n",
    "    for t in range(num_iterations):\n",
    "        \n",
    "        incomplete = True\n",
    "        while incomplete:\n",
    "            try:\n",
    "                scores = []\n",
    "                for p in P: #Calculate rouge scores of all prompts responses  \n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model=\"gpt-3.5-turbo-0613\",\n",
    "                        messages=p\n",
    "                    )\n",
    "                    pred = response['choices'][0]['message']['content']\n",
    "                    p_i_score = rouge_score.compute(\n",
    "                        predictions=[pred],\n",
    "                        references=[target_response]\n",
    "                    )  \n",
    "                    scores.append(p_i_score['rouge1'])\n",
    "                print_generation_best(t, P, argmax=np.argmax(scores))\n",
    "                #perform roulette wheel selection \n",
    "                parents = np.array(P)[roulette_wheel_selection(scores)].tolist()\n",
    "                p1 = str(parents[0])\n",
    "                p2 = str(parents[1])\n",
    "                incomplete = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "        print(f\"Generation {t}: Selection Stage Complete\")\n",
    "        #Crossover\n",
    "        SYS_ROLE = \"You are an AI assistant who completes its given task as closely as possible to the prompt.\"\n",
    "        CROSSOVER_PROMPT = f\"\"\"Given the following two parent prompts:\n",
    "        \n",
    "        Prompt 1: {p1}\n",
    "        \n",
    "        Prompt 2: {p2}\n",
    "        \n",
    "        Your task is to create a new prompt which exactly matches the dictionary format of the originals but changes the content sections.\n",
    "        Change the content sections of your response by combining portions of the original two prompts' content sections.\n",
    "        Importantly, if the example mentions comparing sentences make sure to use the same sentences from the examples in your response.\n",
    "        Do not add new line characters.\n",
    "        \"\"\"\n",
    "        crossover_prompts = []\n",
    "        i = 0\n",
    "        while i < num_prompts:\n",
    "            try:\n",
    "                new_prompt = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo-0613\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": SYS_ROLE}, \n",
    "                        {\"role\": \"user\", \"content\": CROSSOVER_PROMPT}\n",
    "                    ],\n",
    "                    temperature = .99\n",
    "                )\n",
    "                content = new_prompt['choices'][0]['message']['content']\n",
    "                if content != p1 and content != p2:\n",
    "                    crossover_prompts.append(eval(content))\n",
    "                    i += 1\n",
    "            except Exception as e:\n",
    "                print(\"Uninterpretable response generated. Retrying!\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Generation {t}: Crossover Stage Complete\")\n",
    "        #print(crossover_prompts)\n",
    "        #Mutate\n",
    "        mutated_prompts = []\n",
    "        for co_prompt in crossover_prompts:\n",
    "            MUTATE_PROMPT = f\"\"\"Given the following prompt:\n",
    "            {co_prompt}\n",
    "            \n",
    "            Without changing the structure of the dictionary, mutate the following prompt's content sections.\n",
    "            Importantly, if the prompt mentions comparing sentences or statements make sure to use the original sentences or statements in you response.\n",
    "            Change these sections by replacing the words with synonyms or rephrasing ideas.\n",
    "            \"\"\"\n",
    "            incomplete = True\n",
    "            while incomplete:\n",
    "                try:\n",
    "                    new_prompt = openai.ChatCompletion.create(\n",
    "                        model=\"gpt-3.5-turbo-0613\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": SYS_ROLE}, \n",
    "                            {\"role\": \"user\", \"content\": MUTATE_PROMPT}\n",
    "                        ],\n",
    "                        temperature = .99\n",
    "                    )\n",
    "                    content = new_prompt['choices'][0]['message']['content']\n",
    "                    if content != co_prompt:\n",
    "                        mutated_prompts.append(eval(content))\n",
    "                        incomplete = False\n",
    "                except Exception as e:\n",
    "                    print(\"Uninterpretable response generated. Retrying!\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"Generation {t}: Mutation Stage Complete\")\n",
    "        #Evaluation:\n",
    "        for m_prompt in mutated_prompts:\n",
    "            P.append(m_prompt)\n",
    "        \n",
    "        survival_scores = []\n",
    "        for p in P: #Calculate rouge scores of all prompt responses  \n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo-0613\",\n",
    "                messages=p\n",
    "            )\n",
    "            pred = response['choices'][0]['message']['content']\n",
    "            p_i_score = rouge_score.compute(\n",
    "                predictions=[pred],\n",
    "                references=[target_response]\n",
    "            )  \n",
    "            survival_scores.append(p_i_score['rouge1'])\n",
    "            \n",
    "        print(f\"Generation {t}: Evaluation Stage Complete\")\n",
    "        \n",
    "        #Select next generation:\n",
    "        max_indices = np.argsort(survival_scores)\n",
    "        P = np.array(P)[max_indices[3:]].tolist()\n",
    "        print(f\"Generation {t}: Generation Complete. Offspring Selected!\")\n",
    "        \n",
    "        \n",
    "    final_scores = []\n",
    "    final_preds = []\n",
    "    for p in P: #Calculate rouge scores of all prompts responses  \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=p\n",
    "        )\n",
    "        pred = response['choices'][0]['message']['content']\n",
    "        final_preds.append(pred)\n",
    "        p_i_score = rouge_score.compute(\n",
    "            predictions=[pred],\n",
    "            references=[target_response]\n",
    "        )  \n",
    "        final_scores.append(p_i_score['rouge1'])\n",
    "        \n",
    "    print()\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print(f\"Original Prompt:\")\n",
    "    print(\"System Role:\", human_prompt[0]['content'])\n",
    "    print(\"User Prompt:\", human_prompt[1]['content'])\n",
    "    print()\n",
    "    print(\"Response:\", f\"\\n{hp_pred}\")\n",
    "    print(\"Rouge1 Score:\", hp_score)\n",
    "    \n",
    "    final_prompt_idx = np.argmax(final_scores)\n",
    "    content = P[final_prompt_idx]\n",
    "    final_response = final_preds[final_prompt_idx]\n",
    "    print()\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print(f\"Final Prompt:\")\n",
    "    print(\"System Role:\", content[0]['content'])\n",
    "    print(\"User Prompt:\", content[1]['content'])\n",
    "    print()\n",
    "    print(\"Response:\", f\"\\n{final_response}\")\n",
    "    print(\"Rouge1 Score:\", final_scores[final_prompt_idx])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_starting_generation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\danie\\OneDrive\\Desktop\\School\\Fall 2023\\Transformers\\EvoPrompt_Implementation_and_QingyanGuo_Etal_Analysis\\EvoPromptDemo.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/danie/OneDrive/Desktop/School/Fall%202023/Transformers/EvoPrompt_Implementation_and_QingyanGuo_Etal_Analysis/EvoPromptDemo.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m evoprompt_ga(\u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mYes, the two sentences convey the same meaning.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mIn both sentences, the subject is a dog that is actively searching for a buried bone. In both sentences, the dog is described as being eager, excited, and motivated to find the bone. Additionally, both sentences indicate that the bone was buried by the dog and that it had been buried the previous day.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mAlthough the wording and phrasing differ slightly between the two sentences, the overall message and intent of both sentences are the same. The use of synonyms like \u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39mdug up\u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39m and \u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39munearthed,\u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39mfind\u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39m and \u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39mretrieve,\u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39m and \u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39mburied\u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39m and \u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39mstashed away\u001b[39;49m\u001b[39m\\\"\u001b[39;49;00m\u001b[39m adds some variation in phrasing, but the essential meaning remains consistent.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mTherefore, after analyzing the context and synonyms used, it can be concluded that the two sentences convey the same meaning.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\danie\\OneDrive\\Desktop\\School\\Fall 2023\\Transformers\\EvoPrompt_Implementation_and_QingyanGuo_Etal_Analysis\\EvoPromptDemo.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/OneDrive/Desktop/School/Fall%202023/Transformers/EvoPrompt_Implementation_and_QingyanGuo_Etal_Analysis/EvoPromptDemo.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m model_prompts:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/OneDrive/Desktop/School/Fall%202023/Transformers/EvoPrompt_Implementation_and_QingyanGuo_Etal_Analysis/EvoPromptDemo.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     P\u001b[39m.\u001b[39mappend(prompt)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/danie/OneDrive/Desktop/School/Fall%202023/Transformers/EvoPrompt_Implementation_and_QingyanGuo_Etal_Analysis/EvoPromptDemo.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m print_starting_generation(P)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/OneDrive/Desktop/School/Fall%202023/Transformers/EvoPrompt_Implementation_and_QingyanGuo_Etal_Analysis/EvoPromptDemo.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#Step 2: Evolutionary Loop\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/OneDrive/Desktop/School/Fall%202023/Transformers/EvoPrompt_Implementation_and_QingyanGuo_Etal_Analysis/EvoPromptDemo.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'print_starting_generation' is not defined"
     ]
    }
   ],
   "source": [
    "evoprompt_ga(3, 1, \"Yes, the two sentences convey the same meaning.\\n\\nIn both sentences, the subject is a dog that is actively searching for a buried bone. In both sentences, the dog is described as being eager, excited, and motivated to find the bone. Additionally, both sentences indicate that the bone was buried by the dog and that it had been buried the previous day.\\n\\nAlthough the wording and phrasing differ slightly between the two sentences, the overall message and intent of both sentences are the same. The use of synonyms like \\\"dug up\\\" and \\\"unearthed,\\\" \\\"find\\\" and \\\"retrieve,\\\" and \\\"buried\\\" and \\\"stashed away\\\" adds some variation in phrasing, but the essential meaning remains consistent.\\n\\nTherefore, after analyzing the context and synonyms used, it can be concluded that the two sentences convey the same meaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we are going to try to a build a better prompt for:\n",
    "\n",
    "The following are two sentences:\n",
    "\n",
    "- Sentence 1: The dog eagerly dug up the ground to find the bone it had buried yesterday.\n",
    "- Sentence 2: Excited to retrieve what it hid, the dog unearthed the bone it stashed away the previous day.\n",
    "\n",
    "We need to construct a prompt which accomplishes two things. First, do the sentences convey the same meaning, and second explain why or why not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are an AI assistant. You will be given a task. Complete the task and explain in detail how you came to your conclusion.\n",
    "\n",
    "Your task is to determine if the following two sentences convey the same meaning. First, answer either with yes or no, then explain your reasoning. \n",
    "Sentence 1: The dog eagerly dug up the ground to find the bone it had buried yesterday.\n",
    "Sentence 2: Excited to retrieve what it hid, the dog unearthed the bone it stashed away the previous day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EvoPrompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
